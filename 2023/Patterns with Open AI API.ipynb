{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179f2050",
   "metadata": {},
   "source": [
    "## Exploring the Open AI API\n",
    "\n",
    "In this notebook we continue exploring the Open AI API. Through week three of the semester, our content in Text Mining hasn't been too amenable to using Open AI as a tool to actually _do_ the text mining for us. This week we will play around with using the API to do normalization and tokenization and carry out the work in the \"Patterns in Text\" assignment. \n",
    "\n",
    "Once you start using the API, you can start incurring costs. The pricing structure is outlined on the [pricing page](https://openai.com/pricing). Make sure to give it a read. There is a tokenizer tool that you can use [here](https://platform.openai.com/tokenizer). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04403b35",
   "metadata": {},
   "source": [
    "You need to tell the API module about your key. Either method below will work. The environment variable is better, since you don't need to have your key visible in plain text. But feel free to just hard code it in at this point, just be careful putting any code up on GitHub with the api key visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai.api_key = \"sk-VPAJxeOHF7YLCLC0fFT3BljkFJTP5Yux15Rs8FTfrf6Mxj\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If you want to set up the environment variable, try a prompt like this:\n",
    "prompt = \"\"\"\n",
    "My professor has a line of code like this: \n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "Can you tell me how to set this up on my system? \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the example from last class. \n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are some famous astronomical observatories?\"}\n",
    "  ],\n",
    "  temperature=0,\n",
    "  max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00335d97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we'll read in Beowulf and play around with tokenizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/beowulf.txt\", \"r\") as file:\n",
    "    beowulf = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beowulf.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_in_text(text_chunk):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in NLP.\"},\n",
    "            {\"role\": \"user\", \"content\": \n",
    "             f\"\"\"How many tokens are in the following text?\n",
    "                 Please reply with the number of tokens enclosed in brackets.\n",
    "                 Here's an example: \"there are [XXX] tokens in this text.\"\n",
    "                 \n",
    "                 Here's the text:\"\n",
    "                 \\n\\n {text_chunk}\"\"\"}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"] \n",
    "\n",
    "def get_most_common_token(text_chunk, remove_stopwords=True) :\n",
    "    \n",
    "    if remove_stopwords : \n",
    "        user_prompt = f\"\"\"What is the most common token in the following text? \n",
    "                          Please remove stopwords.\\n\\n \\\"{text_chunk}\\\" \"\"\"\n",
    "    else : \n",
    "        user_prompt = f\"What is the most common token in the following text?\\n\\n \\\"{text_chunk}\\\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in NLP.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"] \n",
    "    \n",
    "    \n",
    "def clean_text(text_chunk) :\n",
    "    \n",
    "    user_prompt = f\"\"\"I am going to give you some text. Please perform the following\n",
    "        transformations on the text and return only the transformed text. \n",
    "        \n",
    "        1. Cast everything to lowercase\n",
    "        2. Remove all marks of punctuation\n",
    "        3. Remove stopwords\n",
    "        4. Convert all whitespace characters to spaces\n",
    "        5. Remove any tokens that contain non-alphabetic characters\n",
    "        \n",
    "        Here is the text:\\n\n",
    "         {text_chunk} \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in NLP.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05122d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the text into chunks (e.g., if each chunk is 1000 characters long)\n",
    "chunk_size = 1000\n",
    "text_chunks = [beowulf[i:i+chunk_size] for i in range(0, len(beowulf), chunk_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f97a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens_in_text(text_chunks[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa46a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_chunks[5].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_common_token(text_chunks[2],remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_common_token(text_chunks[1],remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc184e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(text_chunks[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
