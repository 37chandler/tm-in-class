{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('../data/item_list.txt',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full path to this notebook is: /Users/chandler/Library/CloudStorage/Dropbox/teaching/repos/tm-in-class/2023/2023/Zero Shot Learning.ipynb\n"
     ]
    }
   ],
   "source": [
    "notebook_path = os.path.abspath(\"2023/Zero Shot Learning.ipynb\")\n",
    "print(f\"The full path to this notebook is: {notebook_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>FLIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>12P Northwest IPA - Kettlehouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9023</th>\n",
       "      <td>Nino Barraco Vignammare Grillo - 2019 (1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>27M Very Berry Gose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7229</th>\n",
       "      <td>39W Les Vignerones Estezargues \"\"From The Tank\"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5049</th>\n",
       "      <td>07S Pacific Gravity - Firestone Walker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>26S Krieky Bones - Firestone Walker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9351</th>\n",
       "      <td>28S Cherry Duchesse - Verhaeghe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3817</th>\n",
       "      <td>08P Shreddy Day Winter IPA - Draught Works</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Z Beaverslide IPA - Beaverhead Brewing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Item\n",
       "6914                                            FLIGHT\n",
       "3960                   12P Northwest IPA - Kettlehouse\n",
       "9023         Nino Barraco Vignammare Grillo - 2019 (1)\n",
       "8496                               27M Very Berry Gose\n",
       "7229  39W Les Vignerones Estezargues \"\"From The Tank\"\"\n",
       "5049            07S Pacific Gravity - Firestone Walker\n",
       "2770               26S Krieky Bones - Firestone Walker\n",
       "9351                   28S Cherry Duchesse - Verhaeghe\n",
       "3817        08P Shreddy Day Winter IPA - Draught Works\n",
       "245             Z Beaverslide IPA - Beaverhead Brewing"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item    16 Go To Session IPA - Stone\n",
       "Name: 531, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.iloc[531]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the items themselves? What do you notice about the names? \n",
    "\n",
    "--- \n",
    "\n",
    "## Zero Shot Learning\n",
    "\n",
    "Statistical learning is called \"zero shot\" when you're just asking a model (in this case, ChatGPT) to carry out a prediction or estimation function with no training examples. For instance, let's extract the beer name from one of our beers, this one with the item name \"16 Go To Session IPA - Stone\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant.\"\"\"\n",
    "\n",
    "this_beer = items.iloc[531,0]\n",
    "user_prompt = f\"\"\"What beer is in this item name? `{this_beer}`\"\"\"\n",
    "\n",
    "num_tests = 3\n",
    "responses = []\n",
    "\n",
    "for _ in range(num_tests) : \n",
    "\n",
    "    chat_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    responses.append(chat_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The beer in the item name `16 Go To Session IPA - Stone` is the Go To Session IPA by Stone Brewing.',\n",
       " 'The beer in the item name is \"Go To Session IPA\" by Stone Brewing.',\n",
       " 'The beer in the item name `16 Go To Session IPA - Stone` is the Go To Session IPA by Stone Brewing.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start improving our prompt. \n",
    "\n",
    "At this stage we have one main problem. We need the content to come back in a way that we can extract. For instance, if ChatGPT returns 'The beer in the item name `16 Go To Session IPA - Stone` is the Go To Session IPA by Stone Brewing.', it's pretty hard to pull out the beer name. \n",
    "\n",
    "Modify the prompt so that you're getting ChatGPT to consistently return the beer name with some sort of delimiter in it to set off the beer name. Feel free to do this in a cell below so that you can compare the results with the above version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to put your code here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you've got that working, now we have some new things to consider: \n",
    "\n",
    "* Get ChatGPT to extract the beer name with high accuracy. Maybe we should get it to extract the beverage name, since some items aren't beers. \n",
    "* Perhaps we want it to handle things like flights or merch or items that aren't a beverage. \n",
    "\n",
    "As we work on this prompt, it's useful to know how much money we're spending. The pricing can be found [here](https://openai.com/pricing), though it's not in the easiest form to use. For instance, we're working with GPT 3.5 Turbo. That costs \\$0.0015/1K tokens for input and \\$0.002 for output. A token is about 4 characters, but we can look at the actual usage in tokens. Running the below, I used 51 tokens (and probably had similar usage on each of the three requests), so I spent about 150/1000 * \\$0.0015 = \\$0.000225. If I did this 1000 times, it'd cost \\$0.225. Cost might matter _some_ here, because there are ultimately 11K items in our list, though that's still just 11 times the big number below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = chat_response.usage[\"total_tokens\"]\n",
    "print(f\"Your last test cost {num_tests*total_tokens/1000*0.0015}.\")\n",
    "print(f\"If you did this 1000 times it'd be {num_tests*total_tokens*0.0015}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open AI also provides a tokenizer tool if you'd like to play around with how many tokens you're using. You can access it [here](https://platform.openai.com/tokenizer).\n",
    "\n",
    "--- \n",
    "\n",
    "Let's now refine our prompts to try to get some better performance. Rather than just looking at this single item, the code below let's you look at a random selection of `num_items`. Refine your prompt until you're satisfied with its performance. \n",
    "\n",
    "Once you get to that point, run this for, say, 25 beers, and record the fraction that are perfect. This means that, if a beer is sent in, ChatGPT is extracting exactly the beer name in a way that you could easily pull out of the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The item was 17G Royal Fresh Imperial IPA - Deschutes.\n",
      "----- The full is on the next line. -----\n",
      "The beer in the item name is \"Royal Fresh Imperial IPA\" by Deschutes.\n",
      "------------------------------\n",
      "The item was La Grange de Quart Sous Garsinde Red Blend.\n",
      "----- The full is on the next line. -----\n",
      "There is no beer mentioned in the item name `La Grange de Quart Sous Garsinde Red Blend`. It appears to be a red wine blend.\n",
      "------------------------------\n",
      "The item was 36 Frico Rose.\n",
      "----- The full is on the next line. -----\n",
      "Based on the item name `36 Frico Rose`, it does not specify that it is a beer. It is possible that it refers to a different type of beverage or product.\n",
      "------------------------------\n",
      "The item was 22P Slow Elk - Big Sky.\n",
      "----- The full is on the next line. -----\n",
      "The beer in the item name \"22P Slow Elk - Big Sky\" is Big Sky beer.\n",
      "------------------------------\n",
      "The item was 5P Wolf Point Strawberry Wheat - Highlander.\n",
      "----- The full is on the next line. -----\n",
      "The beer in the item name is Wolf Point Strawberry Wheat.\n",
      "------------------------------\n",
      "The item was ZPlaneta Mamertino.\n",
      "----- The full is on the next line. -----\n",
      "The item name `ZPlaneta Mamertino` does not specify a beer. It appears to be a wine called Mamertino produced by the ZPlaneta winery.\n",
      "------------------------------\n",
      "The item was 8 Bullbucker ESB - Cabinet Mt..\n",
      "----- The full is on the next line. -----\n",
      "The beer in the item name \"8 Bullbucker ESB - Cabinet Mt.\" is Bullbucker ESB.\n",
      "------------------------------\n",
      "The item was CHIPS!.\n",
      "----- The full is on the next line. -----\n",
      "I'm sorry, but I cannot determine the specific beer in an item name without more information. Could you please provide more details or context?\n",
      "------------------------------\n",
      "The item was Classic Beer Package.\n",
      "----- The full is on the next line. -----\n",
      "The item name \"Classic Beer Package\" does not specify a specific beer brand or type. It is likely a package that includes a selection of classic beers from various brands.\n",
      "------------------------------\n",
      "The item was 27S Children of the Sun Sour Saison - OddPitch.\n",
      "----- The full is on the next line. -----\n",
      "The beer in the item name is \"Children of the Sun Sour Saison\" by OddPitch.\n",
      "\n",
      "\n",
      "This cost $0.0027.\n",
      "If you did this 1000 times it'd be $2.70\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant.\"\"\"\n",
    "\n",
    "user_prompt_stub = \"\"\"What beer is in this item name?\"\"\"\n",
    "\n",
    "num_items = 10\n",
    "\n",
    "random_items = items.sample(num_items)\n",
    "\n",
    "total_tokens = 0 \n",
    "\n",
    "for item in random_items.itertuples():\n",
    "    this_item = item[1]\n",
    "    user_prompt = user_prompt_stub + f\" `{this_item}`\"\n",
    "\n",
    "    chat_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(f\"The item was {this_item}.\")\n",
    "    print(\"----- The full is on the next line. -----\")\n",
    "    print(chat_response.choices[0].message.content)\n",
    "\n",
    "    total_tokens += chat_response.usage[\"total_tokens\"]\n",
    "\n",
    "\n",
    "print(f\"\\n\\nThis cost ${num_tests*total_tokens/1000*0.0015:.4f}.\")\n",
    "print(f\"If you did this 1000 times it'd be ${num_tests*total_tokens*0.0015:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
