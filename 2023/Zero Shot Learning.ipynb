{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('../data/item_list.txt',sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item list is a list of everything that The Dram Shop has had on tap since inception. Run the next cell a few times to get a sense for the chaos of their naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.iloc[531]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the items themselves? What do you notice about the names? \n",
    "\n",
    "--- \n",
    "\n",
    "## Zero Shot Learning\n",
    "\n",
    "Statistical learning is called \"zero shot\" when you're just asking a model (in this case, ChatGPT) to carry out a prediction or estimation function with no training examples. For instance, let's extract the beer name from one of our beers, this one with the item name \"16 Go To Session IPA - Stone\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant.\"\"\"\n",
    "\n",
    "this_beer = items.iloc[531,0]\n",
    "user_prompt = f\"\"\"What beer is in this item name? `{this_beer}`\"\"\"\n",
    "\n",
    "num_tests = 3\n",
    "responses = []\n",
    "\n",
    "for _ in range(num_tests) : \n",
    "\n",
    "    chat_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    responses.append(chat_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start improving our prompt. \n",
    "\n",
    "At this stage we have one main problem. We need the content to come back in a way that we can extract. For instance, if ChatGPT returns 'The beer in the item name `16 Go To Session IPA - Stone` is the Go To Session IPA by Stone Brewing.', it's pretty hard to pull out the beer name. \n",
    "\n",
    "Modify the prompt so that you're getting ChatGPT to consistently return the beer name with some sort of delimiter in it to set off the beer name. Feel free to do this in a cell below so that you can compare the results with the above version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to put your code here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you've got that working, now we have some new things to consider: \n",
    "\n",
    "* Get ChatGPT to extract the beer name with high accuracy. Maybe we should get it to extract the beverage name, since some items aren't beers. \n",
    "* Perhaps we want it to handle things like flights or merch or items that aren't a beverage. \n",
    "\n",
    "As we work on this prompt, it's useful to know how much money we're spending. The pricing can be found [here](https://openai.com/pricing), though it's not in the easiest form to use. For instance, we're working with GPT 3.5 Turbo. That costs \\$0.0015/1K tokens for input and \\$0.002 for output. A token is about 4 characters, but we can look at the actual usage in tokens. Running the below, I used 51 tokens (and probably had similar usage on each of the three requests), so I spent about 150/1000 * \\$0.0015 = \\$0.000225. If I did this 1000 times, it'd cost \\$0.225. Cost might matter _some_ here, because there are ultimately 11K items in our list, though that's still just 11 times the big number below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = chat_response.usage[\"total_tokens\"]\n",
    "print(f\"Your last test cost {num_tests*total_tokens/1000*0.0015}.\")\n",
    "print(f\"If you did this 1000 times it'd be {num_tests*total_tokens*0.0015}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open AI also provides a tokenizer tool if you'd like to play around with how many tokens you're using. You can access it [here](https://platform.openai.com/tokenizer).\n",
    "\n",
    "--- \n",
    "\n",
    "Let's now refine our prompts to try to get some better performance. Rather than just looking at this single item, the code below let's you look at a random selection of `num_items`. Refine your prompt until you're satisfied with its performance. \n",
    "\n",
    "Once you get to that point, run this for, say, 25 beers, and record the fraction that are perfect. This means that, if a beer is sent in, ChatGPT is extracting exactly the beer name in a way that you could easily pull out of the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant.\"\"\"\n",
    "\n",
    "user_prompt_stub = \"\"\"What beer is in this item name?\"\"\"\n",
    "\n",
    "num_items = 10\n",
    "\n",
    "random_items = items.sample(num_items)\n",
    "\n",
    "total_tokens = 0 \n",
    "\n",
    "for item in random_items.itertuples():\n",
    "    this_item = item[1]\n",
    "    user_prompt = user_prompt_stub + f\" `{this_item}`\"\n",
    "\n",
    "    chat_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(f\"The item was {this_item}.\")\n",
    "    print(\"----- The full is on the next line. -----\")\n",
    "    print(chat_response.choices[0].message.content)\n",
    "\n",
    "    total_tokens += chat_response.usage[\"total_tokens\"]\n",
    "\n",
    "\n",
    "print(f\"\\n\\nThis cost ${num_tests*total_tokens/1000*0.0015:.4f}.\")\n",
    "print(f\"If you did this 1000 times it'd be ${num_tests*total_tokens*0.0015:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
