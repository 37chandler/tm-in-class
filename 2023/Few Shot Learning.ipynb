{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('../data/item_list.txt',sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item list is a list of everything that The Dram Shop has had on tap since inception. Run the next cell a few times to get a sense for the chaos of their naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.iloc[531]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Learning\n",
    "\n",
    "In \"few shot\" learning, we give the AI some examples to follow. The idea is to \"seed\" the model with some examples it can learn from. The performance improvements can be large, as you can see in this [paper](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part#%C2%A7few-shot-learning). In the code below, I'd like you to modify the (not-great) examples I'm giving you to incorporate what you did in the zero-shot learning exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate the prompt engineering you did in Zero Shot Learning.ipynb into \n",
    "# the below prompts. Note that I'm making use of the fact that \n",
    "# python will glue together strings placed within parentheses.\n",
    " \n",
    "system_prompt = \"\"\"You are a world-class Cicerone, trained in beers throughout the world.\"\"\"\n",
    "\n",
    "user_prompt_stub = (\n",
    "  \"I have access to the sales data from a growler fill station. Our data is messy. \"\n",
    "  \"This item description contains the name of a beer, but \"\n",
    "  \"it also contains other information like the tap number, \"\n",
    "  \"the brewery, and maybe other things. I want your help \"\n",
    "  \"extracting the beer names.\\n\\n\"\n",
    "  \"I'm going to give you eight examples of what I'm looking for. \"\n",
    "  \"After these examples, I'll give you an item description. Return \"\n",
    "  \"a line with the same formatting as the examples. The first column \"\n",
    "  \"is the raw item, the second is the cleaned beer name.\\n\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at this prompt in two ways. `pprint` will preserve the returns so it's\n",
    "easier to read. The `print` option will just show these as long strings, which is closer to\n",
    "how you'd send it in to ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(user_prompt_stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_prompt_stub)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make some training examples. Run the below cell until you get at least 10 items where you can figure out how to extract the beer name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 10\n",
    "\n",
    "random_items = items.sample(num_items)['Item'].tolist()\n",
    "\n",
    "random_items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to set up a place for you to put your training data. Once you've filled this in, you'll be able to make the few-shot learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_items = random_items # store these so we can refer to them. \n",
    "\n",
    "clean_items = [\"\"] *len(raw_items)\n",
    "\n",
    "# fill in the above list with your cleaned items. \n",
    "# For instance, if the item was '17G 2 x Thor Double IPA - Melvin', you \n",
    "# might put '2 x Thor Double IPA' in the list. Feel free to shrink raw_items\n",
    "# so that it only has beer in it.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready for our examples. We'll add them on to `user_prompt_stub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(raw_items):\n",
    "\n",
    "  user_prompt_stub += f\"| {item} | {clean_items[idx]} |\\n\"\n",
    "\n",
    "\n",
    "user_prompt_stub += \"Here is the new item: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(user_prompt_stub)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to append the new random items and test the efficacy of this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 10\n",
    "\n",
    "random_items = items.sample(num_items)\n",
    "\n",
    "total_tokens = 0 \n",
    "\n",
    "for item in random_items.itertuples():\n",
    "    this_item = item[1]\n",
    "    user_prompt = user_prompt_stub + f\" |{this_item}|\"\n",
    "\n",
    "    chat_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(f\"The item was {this_item}.\")\n",
    "    print(\"----- The full is on the next line. -----\")\n",
    "    print(chat_response.choices[0].message.content)\n",
    "\n",
    "    total_tokens += chat_response.usage[\"total_tokens\"]\n",
    "\n",
    "\n",
    "print(f\"\\n\\nThis cost ${total_tokens/1000*0.0015:.4f}.\")\n",
    "print(f\"If you did this 1000 times it'd be ${total_tokens*0.0015:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
